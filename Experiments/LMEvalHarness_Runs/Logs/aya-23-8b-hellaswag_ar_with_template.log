

HERE




CHAT TEMPLATE {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 32
hf (pretrained=/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-8B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (32)
|   Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|
|------------|------:|------|-----:|--------|---|-----:|---|-----:|
|hellaswag_ar|      1|none  |     0|acc     |↑  |0.4098|±  |0.0051|
|            |       |none  |     0|acc_norm|↑  |0.5104|±  |0.0052|

