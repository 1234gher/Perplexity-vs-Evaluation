lm_eval --model hf --model_args pretrained='/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-35B' --tasks hellaswag_ar,hellaswag_bn,hellaswag_ca,hellaswag_da,hellaswag_de,hellaswag_es,hellaswag_eu,hellaswag_fr,hellaswag_gu,hellaswag_hi,hellaswag_hr,hellaswag_hu,hellaswag_hy,hellaswag_id,hellaswag_it,hellaswag_kn,hellaswag_ml,hellaswag_mr,hellaswag_ne,hellaswag_nl,hellaswag_pt,hellaswag_ro,hellaswag_ru,hellaswag_sk,hellaswag_sr,hellaswag_sv,hellaswag_ta,hellaswag_te,hellaswag_uk,hellaswag_vi,hellaswag --batch_size auto --write_out --seed 42 --output_path /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/aya-23-35B-hellaswag_with_okapi_without_template --log_samples --wandb_args project=PPL_vs_Eval,name=aya-23-35B-hellaswag_with_okapi_without_template >> /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/Logs/aya-23-35B-hellaswag_with_okapi_without_template.log
