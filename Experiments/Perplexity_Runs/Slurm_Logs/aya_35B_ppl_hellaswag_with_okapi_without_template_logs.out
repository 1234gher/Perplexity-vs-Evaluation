lm_eval --model hf --model_args pretrained='/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-35B' --tasks hellaswag_ar_ppl,hellaswag_bn_ppl,hellaswag_ca_ppl,hellaswag_da_ppl,hellaswag_de_ppl,hellaswag_es_ppl,hellaswag_eu_ppl,hellaswag_fr_ppl,hellaswag_gu_ppl,hellaswag_hi_ppl,hellaswag_hr_ppl,hellaswag_hu_ppl,hellaswag_hy_ppl,hellaswag_id_ppl,hellaswag_it_ppl,hellaswag_kn_ppl,hellaswag_ml_ppl,hellaswag_mr_ppl,hellaswag_ne_ppl,hellaswag_nl_ppl,hellaswag_pt_ppl,hellaswag_ro_ppl,hellaswag_ru_ppl,hellaswag_sk_ppl,hellaswag_sr_ppl,hellaswag_sv_ppl,hellaswag_ta_ppl,hellaswag_te_ppl,hellaswag_uk_ppl,hellaswag_vi_ppl,hellaswag_ppl --batch_size auto --write_out --seed 42 --output_path /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/aya-23-35B-ppl_hellaswag_with_okapi_without_template --log_samples --wandb_args project=PPL_vs_Eval,name=aya-23-35B-ppl_hellaswag_with_okapi_without_template >> /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/Logs/aya-23-35B-ppl_hellaswag_with_okapi_without_template.log
