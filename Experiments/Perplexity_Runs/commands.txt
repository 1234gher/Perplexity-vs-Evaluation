lm_eval --model hf --model_args pretrained='/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-8B' --tasks hellaswag_ar_ppl --device cuda:0 --batch_size auto --write_out --seed 42 --output_path /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/aya-23-8b-hellaswag_ar_ppl --log_samples --wandb_args project=PPL_vs_Eval,name=aya-23-8b-hellaswag_ar_ppl >> /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/Logs/aya-23-8b-hellaswag_ar_ppl.log

lm_eval --model hf --model_args pretrained='/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-8B' --tasks hellaswag_ar_ppl --device cuda:1 --apply_chat_template --batch_size auto --write_out --seed 42 --output_path /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/aya-23-8b-hellaswag_ar_ppl_with_template --log_samples --wandb_args project=PPL_vs_Eval,name=aya-23-8b-hellaswag_ar_ppl_with_template >> /NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/Perplexity_Runs/Logs/aya-23-8b-hellaswag_ar_ppl_with_template.log