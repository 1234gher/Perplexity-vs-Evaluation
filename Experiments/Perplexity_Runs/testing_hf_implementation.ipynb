{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Misc\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Models\"\n",
    "\n",
    "cache_dir = os.getenv(\"TRANSFORMERS_CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T22-30-16.191215.jsonl.feather'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m path_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T22-30-16.191215.jsonl.feather\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m path_no_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_no_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T14-52-52.186900.jsonl.feather\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df_template \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_no_template \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_feather(path_no_template)\n",
      "File \u001b[0;32m/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/pandas/io/feather_format.py:125\u001b[0m, in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401,E501\u001b[39;00m\n\u001b[1;32m    123\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pyarrow_string_dtype():\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m feather\u001b[38;5;241m.\u001b[39mread_feather(\n\u001b[1;32m    130\u001b[0m             handles\u001b[38;5;241m.\u001b[39mhandle, columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(use_threads)\n\u001b[1;32m    131\u001b[0m         )\n",
      "File \u001b[0;32m/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T22-30-16.191215.jsonl.feather'"
     ]
    }
   ],
   "source": [
    "path_template = '/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T22-30-16.191215.jsonl.feather'\n",
    "path_no_template = '/NS/llm-1/work/afkhan/Perplexity-vs-Evaluation/Experiments/LMEvalHarness_Runs/benchmarks_samples/hellaswag_okapi_no_template_aya_23_8B/samples_hellaswag_hi_2024-08-04T14-52-52.186900.jsonl.feather'\n",
    "\n",
    "df_template = pd.read_feather(path_template)\n",
    "df_no_template = pd.read_feather(path_no_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_to_process_template = df_template['gen_args_0.arg_0'].to_list()\n",
    "strings_to_process_no_template = df_no_template['gen_args_0.arg_0'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/NS/llm-1/nobackup/afkhan/Model_Saves/aya-23-8B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673d102c725e422b96804b6672aa56d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CohereForCausalLM(\n",
       "  (model): CohereModel(\n",
       "    (embed_tokens): Embedding(256000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x CohereDecoderLayer(\n",
       "        (self_attn): CohereSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): CohereRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): CohereMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): CohereLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): CohereLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity_on_string(string, model, tokenizer):\n",
    "    # Based on https://huggingface.co/docs/transformers/en/perplexity with stride removed\n",
    "    encodings  = tokenizer(string, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-1] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।\n",
      "Perplexity: 6.714735984802246\n",
      "\n",
      "Input text: Health: अपने बच्चे को दांत पिसने से कैसे रोकें. समझें कि तनाव दांत पिसने का कारण हो सकता है। दांत पिसाई, आपके बच्चे के स्ट्रेस या कुछ चिंता के बारे में परेशान होने का संकेत हो सकता है। अपने बच्चे को आराम दिलाने में मदद करना आपके बच्चे को दांत पिसने से रोकने की कुंजी हो सकता है।\n",
      "Perplexity: 1.0663641691207886\n",
      "\n",
      "Input text: Pets and Animals: अपने बच्चों को कुत्ता चुनने में शामिल करने के लिए कैसे जोड़ें. एक कुत्ते को पाने की संभावना लाओ। अधिकांश बच्चों के लिए कुत्ता पाने का विचार बहुत उत्साहजनक होगा। अपने बच्चों के साथ इस मुद्दे को उठाएं जब वे उत्साहित होने और प्रश्न पूछने के लिए समय हो।\n",
      "Perplexity: 5.237842559814453\n",
      "\n",
      "Input text: High jump: आदमी एक कोण पर दौड़ता है और पीछे की ओर कूदकर छः फुट की बार को पार कर देता है। बाद में\n",
      "Perplexity: 1.1073830127716064\n",
      "\n",
      "Input text: Personal Care and Style: कैसे फिक्रमुक्त रहें. अपने काम का समय और मनोरंजन का समय अलग रखें। जीवन एक मुश्किल नहीं होना चाहिए। अधिक फिक्रमुक्त जीवन जीना चाहते हैं तो दैनिक जीवन में मनोरंजन का समय निकालना और उसे रखना महत्वपूर्ण है।\n",
      "Perplexity: 1.1998382806777954\n",
      "\n",
      "Input text: Food and Entertaining: एक लेयर्ड उपहार बो कैसे बनाएं. व्रैपिंग पेपर से मिलती जुलती रिबन का एक लम्बा टुकड़ा काटें। यह उपहार को लपेटने वाली रिबन से पर्याप्त लम्बी होनी चाहिए और थोड़े से ज़्यादा।. रिबन को उपहार के चारों ओर बाँध दें।\n",
      "Perplexity: 1.9524396657943726\n",
      "\n",
      "Input text: Home and Garden: स्नो पीस को साफ करने का तरीका. स्वस्थ मटर का चयन करें। एक पका हुआ स्नो पी 3 इंच तक लम्बा हो सकता है। उसे इस्तेमाल करने से पहले गलाबंद त्वचा वाले लें।\n",
      "Perplexity: 9158.462890625\n",
      "\n",
      "Input text: Food and Entertaining: सोगी चावल को कैसे ठीक करें. अगर पैन में पानी है, तो पानी को बाकी छोड़ दें और इसे बने रखने दें।  पान के ढक्कन को उतारें ताकि भाप निकल सके। चूल को कम आंच पर सेट करें और चावल को लगभग 5 मिनट तक पकायें।\n",
      "Perplexity: 1.9279766082763672\n",
      "\n",
      "Input text: Camel ride: एक बच्चा रेत को अपने हाथों में छानता है। वीडियो उल्टा हो जाता है जो घोड़ों और ऊँट को पिछले तरफ चलते दिखाता है। ऊँट\n",
      "Perplexity: 1.0027217864990234\n",
      "\n",
      "Input text: Family Life: अपनी टॉडलर को काटने से कैसे रोकें. जानें कि काटना सामान्य है। आपकी टॉडलर आपसे संवाद करने के लिए काट रही है। आपको सीखना होगा कि आपकी टॉडलर क्या कहना चाहती है और फिर उसे ठीक करना होगा।\n",
      "Perplexity: 1.1599555015563965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_no_template = strings_to_process_no_template[:10]\n",
    "\n",
    "results_no_template = []\n",
    "\n",
    "for sample in sample_no_template:\n",
    "    loss = get_perplexity_on_string(sample, model, tokenizer)\n",
    "    results_no_template.append(loss)\n",
    "\n",
    "for input_text, result in zip(sample_no_template, results_no_template):\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Perplexity: {result}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 188811.109375\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Health: अपने बच्चे को दांत पिसने से कैसे रोकें. समझें कि तनाव दांत पिसने का कारण हो सकता है। दांत पिसाई, आपके बच्चे के स्ट्रेस या कुछ चिंता के बारे में परेशान होने का संकेत हो सकता है। अपने बच्चे को आराम दिलाने में मदद करना आपके बच्चे को दांत पिसने से रोकने की कुंजी हो सकता है।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 2538744.0\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Pets and Animals: अपने बच्चों को कुत्ता चुनने में शामिल करने के लिए कैसे जोड़ें. एक कुत्ते को पाने की संभावना लाओ। अधिकांश बच्चों के लिए कुत्ता पाने का विचार बहुत उत्साहजनक होगा। अपने बच्चों के साथ इस मुद्दे को उठाएं जब वे उत्साहित होने और प्रश्न पूछने के लिए समय हो।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 476702.53125\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>High jump: आदमी एक कोण पर दौड़ता है और पीछे की ओर कूदकर छः फुट की बार को पार कर देता है। बाद में<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 582431.125\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: कैसे फिक्रमुक्त रहें. अपने काम का समय और मनोरंजन का समय अलग रखें। जीवन एक मुश्किल नहीं होना चाहिए। अधिक फिक्रमुक्त जीवन जीना चाहते हैं तो दैनिक जीवन में मनोरंजन का समय निकालना और उसे रखना महत्वपूर्ण है।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 374754.09375\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Food and Entertaining: एक लेयर्ड उपहार बो कैसे बनाएं. व्रैपिंग पेपर से मिलती जुलती रिबन का एक लम्बा टुकड़ा काटें। यह उपहार को लपेटने वाली रिबन से पर्याप्त लम्बी होनी चाहिए और थोड़े से ज़्यादा।. रिबन को उपहार के चारों ओर बाँध दें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 507946.84375\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Home and Garden: स्नो पीस को साफ करने का तरीका. स्वस्थ मटर का चयन करें। एक पका हुआ स्नो पी 3 इंच तक लम्बा हो सकता है। उसे इस्तेमाल करने से पहले गलाबंद त्वचा वाले लें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 720104.8125\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Food and Entertaining: सोगी चावल को कैसे ठीक करें. अगर पैन में पानी है, तो पानी को बाकी छोड़ दें और इसे बने रखने दें।  पान के ढक्कन को उतारें ताकि भाप निकल सके। चूल को कम आंच पर सेट करें और चावल को लगभग 5 मिनट तक पकायें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 1055531.0\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Camel ride: एक बच्चा रेत को अपने हाथों में छानता है। वीडियो उल्टा हो जाता है जो घोड़ों और ऊँट को पिछले तरफ चलते दिखाता है। ऊँट<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 242204.140625\n",
      "\n",
      "Input text: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Family Life: अपनी टॉडलर को काटने से कैसे रोकें. जानें कि काटना सामान्य है। आपकी टॉडलर आपसे संवाद करने के लिए काट रही है। आपको सीखना होगा कि आपकी टॉडलर क्या कहना चाहती है और फिर उसे ठीक करना होगा।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity: 572947.6875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_template = strings_to_process_template[:10]\n",
    "\n",
    "results_template = []\n",
    "\n",
    "for sample in sample_template:\n",
    "    loss = get_perplexity_on_string(sample, model, tokenizer)\n",
    "    results_template.append(loss)\n",
    "\n",
    "for input_text, result in zip(sample_template, results_template):\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Perplexity: {result}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template string: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "No template string: Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।\n"
     ]
    }
   ],
   "source": [
    "template_str = sample_template[0]\n",
    "no_template_str = sample_no_template[0]\n",
    "print(f\"Template string: {template_str}\")\n",
    "print(f\"No template string: {no_template_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "<BOS_TOKEN>\n",
      "<|START_OF_TURN_TOKEN|>\n",
      "<|USER_TOKEN|>\n",
      "Personal\n",
      " Care\n",
      " and\n",
      " Style\n",
      ":\n",
      " धन\n",
      "ु\n",
      "ष\n",
      " स\n",
      "े\n",
      " अपन\n",
      "ी\n",
      " फ\n",
      "ै\n",
      "शन\n",
      " व\n",
      "ा\n",
      "र\n",
      "्\n",
      "ड\n",
      "्\n",
      "र\n",
      "ो\n",
      "ब\n",
      " क\n",
      "ो\n",
      " क\n",
      "ै\n",
      "स\n",
      "े\n",
      " अप\n",
      "ड\n",
      "े\n",
      "ट\n",
      " कर\n",
      "ें\n",
      ".\n",
      " ऐस\n",
      "ी\n",
      " ब\n",
      "्\n",
      "ल\n",
      "ा\n",
      "उ\n",
      "ज\n",
      " ख\n",
      "ो\n",
      "ज\n",
      "ें\n",
      " ज\n",
      "ि\n",
      "नक\n",
      "े\n",
      " कम\n",
      " स\n",
      "े\n",
      " कम\n",
      " एक\n",
      " धन\n",
      "ु\n",
      "ष\n",
      " कह\n",
      "ीं\n",
      " पर\n",
      " ह\n",
      "ो।\n",
      " यह\n",
      " ट\n",
      "ॉ\n",
      "प\n",
      " क\n",
      "े\n",
      " ल\n",
      "ि\n",
      "ए\n",
      " सबस\n",
      "े\n",
      " आस\n",
      "ा\n",
      "न\n",
      " व\n",
      "ि\n",
      "कल\n",
      "्\n",
      "प\n",
      " ह\n",
      "ै\n",
      " क\n",
      "्\n",
      "य\n",
      "ों\n",
      "क\n",
      "ि\n",
      " आपक\n",
      "ो\n",
      " अपन\n",
      "े\n",
      " ट\n",
      "ॉ\n",
      "प\n",
      " क\n",
      "े\n",
      " र\n",
      "ं\n",
      "ग\n",
      " और\n",
      " स\n",
      "ा\n",
      "इज\n",
      "़\n",
      " स\n",
      "े\n",
      " म\n",
      "ि\n",
      "लत\n",
      "ी-\n",
      "ज\n",
      "ु\n",
      "लत\n",
      "ी\n",
      " धन\n",
      "ु\n",
      "ष\n",
      " बन\n",
      "ा\n",
      "न\n",
      "े\n",
      " क\n",
      "ी\n",
      " च\n",
      "िं\n",
      "त\n",
      "ा\n",
      " नह\n",
      "ीं\n",
      " करन\n",
      "ी\n",
      " पड\n",
      "़\n",
      "त\n",
      "ी।\n",
      ".\n",
      " धन\n",
      "ु\n",
      "ष\n",
      " म\n",
      "ु\n",
      "द\n",
      "्\n",
      "र\n",
      "ि\n",
      "त\n",
      " ब\n",
      "्\n",
      "ल\n",
      "ा\n",
      "उ\n",
      "ज\n",
      " क\n",
      "ो\n",
      " एक\n",
      " व\n",
      "ि\n",
      "कल\n",
      "्\n",
      "प\n",
      " क\n",
      "े\n",
      " र\n",
      "ू\n",
      "प\n",
      " म\n",
      "ें\n",
      " व\n",
      "ि\n",
      "च\n",
      "ा\n",
      "र\n",
      " कर\n",
      "ें।\n",
      "<|END_OF_TURN_TOKEN|>\n",
      "<|START_OF_TURN_TOKEN|>\n",
      "<|CHATBOT_TOKEN|>\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer(template_str, return_tensors='pt', add_special_tokens=False)\n",
    "print(tokenizer.decode(out['input_ids'][0]))\n",
    "# decode each token in the input_ids\n",
    "for i in out['input_ids'][0]:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer(no_template_str, return_tensors='pt', add_special_tokens=False)\n",
    "print(tokenizer.decode(out['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text (no template): Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: धनुष से अपनी फैशन वार्ड्रोब को कैसे अपडेट करें. ऐसी ब्लाउज खोजें जिनके कम से कम एक धनुष कहीं पर हो। यह टॉप के लिए सबसे आसान विकल्प है क्योंकि आपको अपने टॉप के रंग और साइज़ से मिलती-जुलती धनुष बनाने की चिंता नहीं करनी पड़ती।. धनुष मुद्रित ब्लाउज को एक विकल्प के रूप में विचार करें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.548416256904602\n",
      "Perplexity (template): 1.9173524379730225\n",
      "\n",
      "Input text (no template): Health: अपने बच्चे को दांत पिसने से कैसे रोकें. समझें कि तनाव दांत पिसने का कारण हो सकता है। दांत पिसाई, आपके बच्चे के स्ट्रेस या कुछ चिंता के बारे में परेशान होने का संकेत हो सकता है। अपने बच्चे को आराम दिलाने में मदद करना आपके बच्चे को दांत पिसने से रोकने की कुंजी हो सकता है।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Health: अपने बच्चे को दांत पिसने से कैसे रोकें. समझें कि तनाव दांत पिसने का कारण हो सकता है। दांत पिसाई, आपके बच्चे के स्ट्रेस या कुछ चिंता के बारे में परेशान होने का संकेत हो सकता है। अपने बच्चे को आराम दिलाने में मदद करना आपके बच्चे को दांत पिसने से रोकने की कुंजी हो सकता है।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.1903691291809082\n",
      "Perplexity (template): 1.6922978162765503\n",
      "\n",
      "Input text (no template): Pets and Animals: अपने बच्चों को कुत्ता चुनने में शामिल करने के लिए कैसे जोड़ें. एक कुत्ते को पाने की संभावना लाओ। अधिकांश बच्चों के लिए कुत्ता पाने का विचार बहुत उत्साहजनक होगा। अपने बच्चों के साथ इस मुद्दे को उठाएं जब वे उत्साहित होने और प्रश्न पूछने के लिए समय हो।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Pets and Animals: अपने बच्चों को कुत्ता चुनने में शामिल करने के लिए कैसे जोड़ें. एक कुत्ते को पाने की संभावना लाओ। अधिकांश बच्चों के लिए कुत्ता पाने का विचार बहुत उत्साहजनक होगा। अपने बच्चों के साथ इस मुद्दे को उठाएं जब वे उत्साहित होने और प्रश्न पूछने के लिए समय हो।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.2255840301513672\n",
      "Perplexity (template): 1.644464373588562\n",
      "\n",
      "Input text (no template): High jump: आदमी एक कोण पर दौड़ता है और पीछे की ओर कूदकर छः फुट की बार को पार कर देता है। बाद में\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>High jump: आदमी एक कोण पर दौड़ता है और पीछे की ओर कूदकर छः फुट की बार को पार कर देता है। बाद में<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 2.0028584003448486\n",
      "Perplexity (template): 2.7874159812927246\n",
      "\n",
      "Input text (no template): Personal Care and Style: कैसे फिक्रमुक्त रहें. अपने काम का समय और मनोरंजन का समय अलग रखें। जीवन एक मुश्किल नहीं होना चाहिए। अधिक फिक्रमुक्त जीवन जीना चाहते हैं तो दैनिक जीवन में मनोरंजन का समय निकालना और उसे रखना महत्वपूर्ण है।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Personal Care and Style: कैसे फिक्रमुक्त रहें. अपने काम का समय और मनोरंजन का समय अलग रखें। जीवन एक मुश्किल नहीं होना चाहिए। अधिक फिक्रमुक्त जीवन जीना चाहते हैं तो दैनिक जीवन में मनोरंजन का समय निकालना और उसे रखना महत्वपूर्ण है।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.503532886505127\n",
      "Perplexity (template): 2.1581006050109863\n",
      "\n",
      "Input text (no template): Food and Entertaining: एक लेयर्ड उपहार बो कैसे बनाएं. व्रैपिंग पेपर से मिलती जुलती रिबन का एक लम्बा टुकड़ा काटें। यह उपहार को लपेटने वाली रिबन से पर्याप्त लम्बी होनी चाहिए और थोड़े से ज़्यादा।. रिबन को उपहार के चारों ओर बाँध दें।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Food and Entertaining: एक लेयर्ड उपहार बो कैसे बनाएं. व्रैपिंग पेपर से मिलती जुलती रिबन का एक लम्बा टुकड़ा काटें। यह उपहार को लपेटने वाली रिबन से पर्याप्त लम्बी होनी चाहिए और थोड़े से ज़्यादा।. रिबन को उपहार के चारों ओर बाँध दें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.6901707649230957\n",
      "Perplexity (template): 2.164479970932007\n",
      "\n",
      "Input text (no template): Home and Garden: स्नो पीस को साफ करने का तरीका. स्वस्थ मटर का चयन करें। एक पका हुआ स्नो पी 3 इंच तक लम्बा हो सकता है। उसे इस्तेमाल करने से पहले गलाबंद त्वचा वाले लें।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Home and Garden: स्नो पीस को साफ करने का तरीका. स्वस्थ मटर का चयन करें। एक पका हुआ स्नो पी 3 इंच तक लम्बा हो सकता है। उसे इस्तेमाल करने से पहले गलाबंद त्वचा वाले लें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 2.2835376262664795\n",
      "Perplexity (template): 2.7622039318084717\n",
      "\n",
      "Input text (no template): Food and Entertaining: सोगी चावल को कैसे ठीक करें. अगर पैन में पानी है, तो पानी को बाकी छोड़ दें और इसे बने रखने दें।  पान के ढक्कन को उतारें ताकि भाप निकल सके। चूल को कम आंच पर सेट करें और चावल को लगभग 5 मिनट तक पकायें।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Food and Entertaining: सोगी चावल को कैसे ठीक करें. अगर पैन में पानी है, तो पानी को बाकी छोड़ दें और इसे बने रखने दें।  पान के ढक्कन को उतारें ताकि भाप निकल सके। चूल को कम आंच पर सेट करें और चावल को लगभग 5 मिनट तक पकायें।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.9048559665679932\n",
      "Perplexity (template): 2.428893804550171\n",
      "\n",
      "Input text (no template): Camel ride: एक बच्चा रेत को अपने हाथों में छानता है। वीडियो उल्टा हो जाता है जो घोड़ों और ऊँट को पिछले तरफ चलते दिखाता है। ऊँट\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Camel ride: एक बच्चा रेत को अपने हाथों में छानता है। वीडियो उल्टा हो जाता है जो घोड़ों और ऊँट को पिछले तरफ चलते दिखाता है। ऊँट<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 2.143597364425659\n",
      "Perplexity (template): 3.082315683364868\n",
      "\n",
      "Input text (no template): Family Life: अपनी टॉडलर को काटने से कैसे रोकें. जानें कि काटना सामान्य है। आपकी टॉडलर आपसे संवाद करने के लिए काट रही है। आपको सीखना होगा कि आपकी टॉडलर क्या कहना चाहती है और फिर उसे ठीक करना होगा।\n",
      "Input text (template): <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Family Life: अपनी टॉडलर को काटने से कैसे रोकें. जानें कि काटना सामान्य है। आपकी टॉडलर आपसे संवाद करने के लिए काट रही है। आपको सीखना होगा कि आपकी टॉडलर क्या कहना चाहती है और फिर उसे ठीक करना होगा।<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Perplexity (no template): 1.5211552381515503\n",
      "Perplexity (template): 2.1053669452667236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_text_no_template, input_text_template, result_no_template, result_template in zip(sample_no_template, sample_template, results_no_template, results_template):\n",
    "    print(f\"Input text (no template): {input_text_no_template}\")\n",
    "    print(f\"Input text (template): {input_text_template}\")\n",
    "    print(f\"Perplexity (no template): {result_no_template}\")\n",
    "    print(f\"Perplexity (template): {result_template}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: lorem ipsum\n",
      "Perplexity: 150.28932189941406\n",
      "\n",
      "Input text: Happy Birthday!\n",
      "Perplexity: 189.37318420410156\n",
      "\n",
      "Input text: Bienvenue\n",
      "Perplexity: 176.8068389892578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
    "results = []\n",
    "for input_text in input_texts:\n",
    "    results.append(get_perplexity_on_string(input_text, model, tokenizer))\n",
    "\n",
    "for input_text, result in zip(input_texts, results):\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Perplexity: {result}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: git pull origin master fetches and merges the contents of the master branch with your branch and creates a merge commit. If there are any merge conflicts you'll be notified at this stage and you must resolve the merge commits before proceeding. When you are ready to push your local commits, including your new merge commit, to the remote server, run git push.\n",
      "Perplexity: 11.2291259765625\n",
      "\n",
      "Input text: Some evaluation modules require some external data such as NLTK that requires resources or the BLEURT metric that requires checkpoints. You can implement these downloads in EvaluationModule._download_and_prepare(), which downloads and caches the resources via the dlmanager. A simplified example on how BLEURT downloads and loads a checkpoint\n",
      "Perplexity: 68.48452758789062\n",
      "\n",
      "Input text: Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life.\n",
      "Perplexity: 5.6939377784729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"git pull origin master fetches and merges the contents of the master branch with your branch and creates a merge commit. If there are any merge conflicts you'll be notified at this stage and you must resolve the merge commits before proceeding. When you are ready to push your local commits, including your new merge commit, to the remote server, run git push.\",\n",
    "                \"Some evaluation modules require some external data such as NLTK that requires resources or the BLEURT metric that requires checkpoints. You can implement these downloads in EvaluationModule._download_and_prepare(), which downloads and caches the resources via the dlmanager. A simplified example on how BLEURT downloads and loads a checkpoint\", \n",
    "                \"Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life.\"]\n",
    "results = []\n",
    "\n",
    "for input_text in input_texts:\n",
    "    results.append(get_perplexity_on_string(input_text, model, tokenizer))\n",
    "\n",
    "for input_text, result in zip(input_texts, results):\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Perplexity: {result}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_mem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
